# merge variables into dataframe
viagraData<-data.frame(dose, libido, partnerLibido)
viagraData<-read.delim("ViagraCovariate.dat", header = TRUE)
viagraData<-read.delim("ViagraCovariate.dat", header = TRUE)
viagraData<-read.delim("C:\Users/sherr/School/Sp20/Psych101/Reading 101/ViagraCovariate.dat", header = TRUE)
viagraData<-read.delim("C:/Users/sherr/School/Sp20/Psych101/Reading 101/ViagraCovariate.dat", header = TRUE)
viagraData<-read.delim("C:/Users/sherr/School/Sp20/Psych101/Reading 101/ViagraCovariate.dat", header = TRUE)
# directly creating data
libido<-c(3,2,5,2,2,2,7,2,4,7,5,3,4,4,7,5,4,9,2,6,3,4,4,4,6,4,6,2,8,5)
partnerLibido < - c ( 4 , 1 , 5 , 1 , 2 , 2 , 7 , 4 , 5 , 5 , 3 , 1 , 2 , 2 , 6 , 4 , 2 , 1 , 3 , 5 , 4,3,3,2,0,1,3,0,1,0)
dose<-c(rep(1,9),rep(2,8), rep(3,13)) # repeats number to represent groups
# signifying + labeling levels
dose<-factor(dose, levels = c(1:3), labels = c("Placebo", "Low Dose", "High Dose"))
# merge variables into dataframe
viagraData<-data.frame(dose, libido, partnerLibido)
partnerLibido < - c ( 4 , 1 , 5 , 1 , 2 , 2 , 7 , 4 , 5 , 5 , 3 , 1 , 2 , 2 , 6 , 4 , 2 , 1 , 3 , 5 , 4,3,3,2,0,1,3,0,1,0)
viagraData<-read.delim("C:/Users/sherr/School/Sp20/Psych101/Reading 101/ViagraCovariate.dat", header = TRUE)
# directly creating data
libido<-c(3,2,5,2,2,2,7,2,4,7,5,3,4,4,7,5,4,9,2,6,3,4,4,4,6,4,6,2,8,5)
partnerLibido <- c ( 4 , 1 , 5 , 1 , 2 , 2 , 7 , 4 , 5 , 5 , 3 , 1 , 2 , 2 , 6 , 4 , 2 , 1 , 3 , 5 , 4,3,3,2,0,1,3,0,1,0)
dose<-c(rep(1,9),rep(2,8), rep(3,13)) # repeats number to represent groups
# signifying + labeling levels
dose<-factor(dose, levels = c(1:3), labels = c("Placebo", "Low Dose", "High Dose"))
# merge variables into dataframe
viagraData<-data.frame(dose, libido, partnerLibido)
# descriptive statistics
by(viagraData$libido, viagraData$dose, stat.desc)
by(viagraData$partnerLibido, viagraData$dose, stat.desc)
install.packages("car"); install.packages("compute.es"); install.packages
("effects"); install.packages("ggplot2"); install.packages("multcomp"); install.
packages("pastecs"); install.packages("WRS", repos="http://R-Forge.R-project.
org")
install.packages("effects")
install.packages("effects")
install.packages("car")
install.packages("compute.es")
install.packages("effects")
install.packages("ggplot2")
install.packages("multcomp")
install.packages("pastecs")
install.packages("WRS", repos="http://R-Forge.R-project.org")
by(viagraData$libido, viagraData$dose, stat.desc)
by(viagraData$partnerLibido, viagraData$dose, stat.desc)
by(viagraData$libido, viagraData$dose, stat.desc)
by(viagraData$libido, viagraData$dose, stat.desc)
by(viagraData$partnerLibido, viagraData$dose, stat.desc)
by(viagraData$libido, viagraData$dose, stat.desc)
by(viagraData$partnerLibido, viagraData$dose, summary)
by(viagraData$libido, viagraData$dose, summary)
by(viagraData$partnerLibido, viagraData$dose, summary)
library(car)
library(compute.es)
library(effects)
library(ggplot2)
library(multcomp)
library(pastecs)
library(WRS)
library(WRS2)
by(viagraData$libido, viagraData$dose, stat.desc)
by(viagraData$partnerLibido, viagraData$dose, stat.desc)
leveneTest(viagraData$libido, viagraData$dose, center = median)
# nonsignificant F
leveneTest(viagraData$libido, viagraData$dose, center = median)
viagraModel<-aov(libido ~ dose, data = viagraData)
# add new predictor
viagraModel<-aov(libido ~ dose + partnerLibido, data = viagraData)
# order matters!
# nonsignifcant effect of covariate, significant effect of dose
covariateFirst<-aov(libido ~ partnerLibido + dose, data = viagraData)
summary(covariateFirst)
covariateFirst<-aov(libido ~ partnerLibido + dose, data = viagraData)
summary(covariateFirst)
doseFirst<-aov(libido ~ dose + partnerLibido, data = viagraData) summary(doseFirst)
doseFirst<-aov(libido ~ dose + partnerLibido, data = viagraData)
summary(doseFirst)
Anova(doseFirst, type = "III")
Anova(covariateFirst, type = "III")
viagraModel<-aov(libido ~ partnerLibido + dose, data = viagraData)
# set contrast codes: orthagonal
contrasts(viagraData$dose)<-contr.helmert(3)
contrasts(viagraData$dose)<-cbind(c(-2,1,1), c(0,-1,1))
# Type III ANCOVA
contrasts(viagraData$dose)<-cbind(c(-2,1,1), c(0,-1,1))
viagraModel<-aov(libido ~ partnerLibido + dose, data = viagraData)
# significant effect of tmt after covariate removal
Anova(viagraModel, type="III")
adjustedMeans<-effect("dose", viagraModel, se=TRUE)
summary(adjustedMeans)
adjustedMeans$se
postHocs<-glht(viagraModel, linfct = mcp(dose = "Tukey"))
summary(postHocs)
confint(postHocs)
# residuals vs. fitted: homogeneity of variance
# Q-Q plot: normality of residuals (should be on line)
plots(viagraModel)
install.packages("WRS", repos="http://R-Forge.R-project.org")
library(WRS2)
library(WRS)
plot(viagraModel)
# colon indivates interaction
hoRS<-aov(libido ~ partnerLibido + dose + dose:partnerLibido, data = viagraData)
# considers individual effects + interaction
hoRS<-aov(libido ~ partnerLibido*dose, data = viagraData)
# update original ANCOVA with interaction term
# .~. = keep original outcome/predictor, add interaction term
hoRS<-update(viagraModel, .~. + partnerLibido:dose)
# significant interaction: assumptions of ANCOVA not met
Anova(hoRS, type="III")
library(car) # make sure you have this package installed first
presto <- Prestige
## Q1. How many individuals are in this dataset
nrow(presto)
## Q2. How many variables are in this dataset?
length(presto)
## Q3. What is the name of the fifth variable in the dataset?
names(presto)
## Q4. What is the value of the 42nd row and 5th variable?
presto[42, 5]
## Q5. What is the value of the 42nd row and 13th variable?
presto[42, 13]
## Q6. What is the average (mean) of the variable 'education'?
mean(presto$education, na.rm = T)
## Q7. What is the average (mean) of the variable income?
mean(presto$income, na.rm = T)
## Q8. How would you describe the shape of the distribution of the variable income?
hist(presto$income)
resid <- presto$income - inc_mean
resid_sq <- resid^2
sum(resid_sq)
resid <- presto$income - inc_mean
inc_mean <- mean(presto$income, na.rm = T)
resid <- presto$income - inc_mean
resid_sq <- resid^2
sum(resid_sq)
prestige2[prestige2$income > 20000 & !is.na(prestige2$income), ]$income
prestige2 <- presto
prestige2[prestige2$income > 20000 & !is.na(prestige2$income), ]$income
prestige2[prestige2$income > 20000 & !is.na(prestige2$income), ]$income <- NA
## Q11. What is the mean of the variable 'income' when you remove this individual?
mean(prestige2$income)
## Q11. What is the mean of the variable 'income' when you remove this individual?
mean(prestige2$income, na.rm = T)
mean(INC, na.rm = T)
library(psy)
library(psych)
describe(class)
install.packages('DSUR')
install.packages('DSUR.noof')
install.packages("devtools")
library(devtools)
install_github("Frostarella/DSUR.noof")
t<-c(2.227, 2.785, 0.541)
df<-26
# DSUR package
rcontrast(t, df) # medium, med, small respectively
library(DSUR.noof)
t<-c(2.227, 2.785, 0.541)
df<-26
# DSUR package
rcontrast(t, df) # medium, med, small respectively
library(calculate.es)
install.packages(calculate.es)
install.packages("calculate.es")
library(compute.es)
mes(5.988117, 4.151886, 1.755879, 1.788613, 8, 9)
knitr::opts_chunk$set(echo = TRUE)
setwd('/Users/sherr/School/Sp20/Psych101/101 Lab/Lab 6/')
anchor <- read.csv('anchoring_SP20.csv')
plot(anchor$cond)
plot(anchor$meat)
plot(anchor$tree)
anchor$meat[anchor$meat < 800000]
anchor$meat[anchor$meat > 800000] <- NA
anchor$meat[anchor$meat > 800000] <- NA
plot(anchor$meat)
anchor[anchor$meat > 800000]$meat <- NA
plot(anchor$cat)
anchor <- ifelse(anchor$meat > 800000, NA, anchor$cat)
plot(anchor$cat)
anchor <- ifelse(anchor$meat > 800000, NA, anchor$meat)
anchor$meat[anchor$meat > 800000]
setwd('/Users/sherr/School/Sp20/Psych101/101 Lab/Lab 6/')
anchor <- read.csv('anchoring_SP20.csv')
anchor$meat[anchor$meat > 800000]
anchor$meat[anchor$meat > 800000] <- NA
plot(anchor$meat)
anchor$meat[anchor$meat > 10000] <- NA
anchor$meat[anchor$meat > 10000] <- NA
plot(anchor$meat)
plotmeans(meat ~ cond, anchor)
library(gplots)
plotmeans(meat ~ cond, anchor)
library(gplots)
plotmeans(meat ~ cond, anchor)
plotmeans(tree ~ cond, anchor)
tree_mod <- lm(tree ~ cond, anchor)
meat_mod <- lm(meat ~ cond, anchor)
tree_mod$coefficients
meat_mod$coefficients
662.7816 + -464.2002 * 1
# prediction for tree, high
662.7816 + -464.2002 * 0
# prediction for meat, low
1030.9059 + -879.8367 * 1
# prediction for tree, high
1030.9059 + -879.8367 * 0
summary(meat_mod)$R_squared
summary(meat_mod)$r_squared
summary(meat_mod)$r.squared
summary(tree_mod)$r.squared
chick <- read.csv('chicken_weights.csv')
names(chick)
chick$feed
plot(chick$feed)
plot(chick$X)
plot(chick$feed)
plot(chick$weight)
chick_mod <- lm(weight ~ feed, chick)
plotmeans(weight ~ feed, chick)
summary(chick_mod)$r.squared
plot(chicken$feedR)
chicken$feedR <- relevel(chicken$feed, ref = "meatmeal")
chick$feedR <- relevel(chick$feed, ref = "meatmeal")
plot(chick$feedR)
plotmeans(weight ~ feedR, chick)
class <- read.csv('class_dataset_SP20.csv')
levels(class$postcollege)
levels(class$diet)
max(class$o3r)
min(class$o3r)
open <- with(student, data.frame(o1, o2, 6 - o3r))
class <- read.csv('class_dataset_SP20.csv')
levels(class$diet)
levels(class$diet)[1] <- NA
max(class$o3r)
min(class$o3r)
open <- with(class$diet, data.frame(o1, o2, 6 - o3r))
class <- read.csv('class_dataset_SP20.csv')
levels(class$diet)
levels(class$diet)[1] <- NA
max(class$o3r)
min(class$o3r)
open <- with(class, data.frame(o1, o2, 6 - o3r))
class$openness <- rowMeans(open)
class$openness
plot(class$openness)
plot(class$diet)
diet_open <- lm(openness ~ diet, class)
coef(diet_open)
plotmeans(openness ~ diet, class)
summary(diet_open)$r.squared
mod4 <- lm(prestige ~ education * type, presto)
library(car)
library(arm)
library(rgl)
presto <- Prestige
mod <- lm(prestige ~ income, presto)
coef(mod)
mod2 <- lm(prestige ~ education, data = presto)
mod3 <- lm(prestige ~ education + income, presto)
coef(mod)
# education changes less, so more important
standardize(mod, NULL, T) # bivariate rlsp btwn inc/prestige
standardize(mod3, NULL, T) # multivariate rlsp btwn education/prestige, controlling for income
standardize(mod2, NULL, T) # bivariate rlsp btwn ed/prestige
# R2: inc + educ explains 80% of variation
summary(mod3)
# educ explains 72% of variation
summary(mod2)
# income explains 50% of variation
summary(mod)
# adjusted R2: penalty for adding new varialbles
# overfitting: high R2
# if large multiple/adjusted R2 diff
scatter3d(prestige ~ education + income, presto)
mod4 <- lm(prestige ~ education * type, presto)
standardize(mod4, NULL, T)
cf <- coef(mod4)
plot(scale(prestige) ~ scale(education), presto, col = presto$type, pch = 19)
standardize(mod4, NULL, T)
abline(a = cf[1], b = cf[2], lwd = 5, col = 'black') # bc slope/int
cf <- coef(standardize(mod4, NULL, T))
cf <- coef(standardize(mod4, NULL, T))
plot(scale(prestige) ~ scale(education), presto, col = presto$type, pch = 19)
abline(a = cf[1], b = cf[2], lwd = 5, col = 'black') # bc slope/int
abline(a = cf[1] + cf[3], b = cf[2] + cf[5], lwd = 5, col = 'red')
abline(a = cf[1] + cf[4], b = cf[2] + cf[6], lwd = 5, col = 'red')
abline(a = cf[1] + cf[4], b = cf[2] + cf[6], lwd = 5, col = 'green')
plot(scale(prestige) ~ scale(education), presto, col = presto$type, pch = 19)
abline(a = cf[1], b = cf[2], lwd = 3, col = 'black') # bc slope/int
abline(a = cf[1] + cf[3], b = cf[2] + cf[5], lwd = 3, col = 'red')
abline(a = cf[1] + cf[4], b = cf[2] + cf[6], lwd = 3, col = 'green')
summary(mod4)
summary(standardize(mod4, NULL, T))
summary(standardize(mod4, NULL, T))
hist(placebo$age, col = 'Cond')
setwd('/Users/sherr/School/Sp20/Psych101/Final Project/Data Analysis/')
placebo <- read.csv("Psych 101 Project Survey - Final_April 17, 2020_17.53.csv", header=T, na.strings=c(""))
head(placebo)[,17:30]
nrow(placebo)
# libraries
library(dplyr)
library(psych)
library(gplots)
library(arm)
library(scales)
# correct responses to memory tests
correct1 <- c('BEJ',
'BUW',
'DIJ',
'FEP',
'FUP',
'JEX',
'JIH',
'KOJ',
'MEP',
'NAF'
)
correct2 <- c('BIW',
'DEJ',
'FAJ',
'FOV',
'HAJ',
'JID',
'KEF',
'MAF',
'MIV',
'NUV'
)
# removing unnecessary columns
names(placebo)
placebo <- placebo[, 18:47]
# results of 2 memory tests
names(placebo)
test1_df <- placebo[, 1:10]
test2_df <- placebo[, 11:20]
test1_df[] <- lapply(test1_df, as.character)
test2_df[] <- lapply(test2_df, as.character)
# turning empty responses into incorrect ones
test1_df[is.na(test1_df)] <- 'WRONG'
test2_df[is.na(test2_df)] <- 'WRONG'
test1_df
test2_df
# converting all responses to uppercase only
test1_df <- data.frame(lapply(test1_df, function(v) {
if (is.character(v)) return(toupper(v))
else return(v)
}))
test2_df <- data.frame(lapply(test2_df, function(v) {
if (is.character(v)) return(toupper(v))
else return(v)
}))
# creating 2 vectors of correctness scores
test1_scores <- array()
for (r in c(1:nrow(test1_df))) {
correct <- 0
for (i in c(1:10)) {
correct <- correct + (test1_df[r, i] %in% correct1)
}
test1_scores[r] <- correct
}
test2_scores <- array()
for (r in c(1:nrow(test2_df))) {
correct <- 0
for (i in c(1:10)) {
correct <- correct + (test2_df[r, i] %in% correct2)
}
test2_scores[r] <- correct
}
length(test1_scores)
length(test2_scores)
test1_scores
test2_scores
# checking to make sure scores actually correspond to reality
tail(test1_df)
# merging scores vectors with original dataframe
names(placebo)
placebo$test1 <- test1_scores
placebo$test2 <- test2_scores
head(placebo)
# removing all unecessary columns
names(placebo)
placebo <- with(placebo, data.frame(Q8, Q9, Q10, Q11, Q12, Q13, FL_6_DO, test1, test2))
head(placebo)
# combining test scores
test_comb <- cbind(placebo$test1, placebo$test2)
placebo$test_tot <- placebo$test1 + placebo$test2
alpha(test_comb)
# renaming columns to be more informative
placebo <- placebo %>% rename(Age = Q8, Sex = Q9, Ed = Q10, PsychClass = Q11, KnowPlacebo = Q12, PlaceboFamiliarity = Q13, Cond = FL_6_DO)
# removing participant 19 who didn't complete the survey
placebo <- placebo[-c(19), ]
tail(placebo)
# shortening education levels
levels(placebo$Ed) <- c('college', 'grad', 'HS', 'no HS')
placebo$Ed
# changing psych knowledge levels to numeric
# normalized so all equally impactful
levels(placebo$PsychClass) <- c(0, 4, 2)
levels(placebo$PlaceboFamiliarity) <- c(4, 2, 0, 1, 3)
# scoring psych knowledge
psych_know <- cbind(placebo$PsychClass, placebo$PlaceboFamiliarity)
placebo$PsychKnow <- rowMeans(psych_know)
head(placebo)
alpha(psych_know) # this is bad :(
# alternative: using as categorical variable; not continuous scale
# splitting by condition
ctrl <- subset(placebo, Cond == 'Block1')
nrow(ctrl)
tmt <- subset(placebo, Cond == 'Block4')
nrow(tmt)
levels(placebo$Cond) <- c('C', 'T')
# not an even split, but so what?
# FINALLY DATA CLEANING IS DONE
bivar <- standardize(lm(test_tot ~ Cond, data = placebo), NULL, T)
bivar2 <- standardize(lm(test_tot ~ PsychKnow, placebo), NULL, T)
par(mar=c(2, 2, 2, 2))
plotmeans(scale(test_tot) ~ Cond, data = placebo,
main = 'Condition vs. Memory', ylab = 'Memory Score', cex.axis = 0.75, cex.main = 0.75, cex.lab = 0.75)
# people in treatment group performed worse??!! but nonsignificant
# treatment performed worse, but nonsignificant
# R2: explains about 2% of variation in memory scores
summary(bivar)
# seems like 0 relationship
par(mar=c(4, 4, 2, 1))
plot(scale(placebo$PsychKnow), scale(placebo$test_tot),
main = 'PK vs. Memory Performance',
xlab = 'PK', ylab = 'Memory',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)# yep, slope is basically 0, and p value is almost 1 lol
abline(bivar2, lwd = 3, col = 'red')
summary(bivar2)
# people who knew about placebo effect
# WOW
# condt:psycknow: adjust slope up by 1.3 for each pt increase in psychknow btwn tmt/ctrl
# people who knew about placebo had MORE improvement in memory btwn tmt/contrl
# p = 0.006!!!!
know_mod <- standardize(lm(test_tot ~ Cond * PsychKnow, data = placebo), NULL, T)
summary(know_mod)
# plotting diff btwn tmt/ctrl at diff levels of psychknow
# at higher levels of psychknow a diff btwn results emerges
par(mar = c(4, 4, 2, 1))
plot(scale(placebo$PsychKnow), scale(placebo$test_tot), col = placebo$Cond, pch = 19,
main = 'Treatment vs. Control Across PK Levels',
xlab = 'PK', ylab = 'Memory',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
legend(x = 'topright', legend = c('T', 'C'), col=c("red", "black"),
pch = c(19, 19), cex = 0.8)
tmt_pk <- standardize(lm(test_tot ~ PsychKnow, data = tmt), NULL, T)
ctrl_pk <- standardize(lm(test_tot ~ PsychKnow, data = ctrl), NULL, T)
abline(tmt_pk, col = alpha('red', 0.5), lwd = 3, lty = 'longdash')
abline(ctrl_pk, col = alpha('black', 0.5), lwd = 2, lty = 'longdash')
summary(tmt_pk)
summary(ctrl_pk)
# wow that's INTERSTING
# demographic assessments
describe(placebo$Age)
summary(placebo$Sex)
describe(tmt$Age)
summary(tmt$Sex)
summary(placebo$Ed)
supp
describe(ctrl$Age)
summary(ctrl$Sex)
describe(placebo)
summary(placebo$KnowPlacebo)
hist(placebo$age, col = Cond)
hist(placebo$Age, col = Cond)
hist(placebo$Age)
hist(placebo$Age, col = placebo$Cond,
main = 'Age Distribution',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(placebo$Age, col = alpha(placebo$Cond, 0.5)
main = 'Age Distribution',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(placebo$Age, col = alpha(placebo$Cond, 0.5),
main = 'Age Distribution',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(tmt$Age,
main = 'Age Distribution (T)',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(ctrl$Age,
main = 'Age Distribution (C)',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(tmt$Age,
main = 'Age Distribution (T)',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(ctrl$Age,
main = 'Age Distribution (C)',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(tmt$PsychKnow,
main = 'PK (T)',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(ctrl$PsychKnow,
main = 'PK (C)',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(tmt$PsychKnow,
main = 'PK (T)',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
hist(ctrl$PsychKnow,
main = 'PK (C)',
xlab = 'Age',
cex.main = 0.75, cex.lab = 0.75, cex.axis = 0.75)
